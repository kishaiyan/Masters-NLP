{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id               author        date  year month         topic  \\\n",
      "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
      "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
      "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
      "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
      "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
      "\n",
      "                                             article  \n",
      "0  PARIS  ?   When the Islamic State was about to...  \n",
      "1  Angels are everywhere in the Mu?iz family?s ap...  \n",
      "2  Finally. The Second Avenue subway opened in Ne...  \n",
      "3  WASHINGTON  ?   It?s   or   time for Republica...  \n",
      "4  For Megyn Kelly, the shift from Fox News to NB...  \n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('news_dataset.csv',encoding='iso-8859-1')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Counts:\n",
      "politics         324\n",
      "business         208\n",
      "entertainment    153\n",
      "crime            110\n",
      "lifestyle         78\n",
      "law               41\n",
      "sports            30\n",
      "science           25\n",
      "technology        18\n",
      "architecture       4\n",
      "accidents          4\n",
      "art                2\n",
      "health             2\n",
      "environment        1\n",
      "Name: topic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "topic_counts = data['topic'].value_counts()\n",
    "\n",
    "# Print the count of every topic\n",
    "print(\"Topic Counts:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    # Split the document into words\n",
    "    words = document.split()\n",
    "\n",
    "    # Check if the first word is all uppercase\n",
    "    if words[0].isupper():\n",
    "        # Find the index of the first question mark if it exists\n",
    "        try:\n",
    "            first_question_index = words.index('?')\n",
    "        except ValueError:\n",
    "            # '?' not found, set first_question_index to the length of the list\n",
    "            first_question_index = len(words)\n",
    "\n",
    "        # Remove words until the first question mark (inclusive)\n",
    "        words = words[first_question_index:]\n",
    "\n",
    "    # Join the remaining words back into a string\n",
    "    result = ' '.join(words)\n",
    "    # Check if the first character of the result is not '?'\n",
    "    if result and result[0] != '?':\n",
    "        return result\n",
    "    else:\n",
    "        # Return the result without the first character if it starts with '?'\n",
    "        return result[1:]\n",
    "\n",
    "\n",
    "# Assuming data is a DataFrame containing articles in a column named 'article'\n",
    "# Apply process_document to all documents in the 'article' column\n",
    "data['processed_article'] = data['article'].apply(process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['processed_article'] = data['processed_article'].str.replace(\"?\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" When the Islamic State was about to be driven out of the ancient city of Palmyra in March, Yves Ubelmann got a call from Syria's director of antiquities to come over in a hurry. An architect by training, Mr. Ubelmann, 36, had worked in Syria before the country was engulfed by war. But now there was special urgency for the kind of work his youthful team of architects, mathematicians and designers did from their cramped offices in Paris: producing digital copies of threatened historical sites. Palmyra, parts of it already destroyed by the Islamists who deemed these monuments idolatrous, was still rigged with explosives. So he and Houmam Saad, his Syrian colleague, spent four days flying a drone with a robot camera over the crumbled arches and temples. 'Drones with four or six rotors can hover really close and register structural details, every crack and hole, and we can take very precise measurements,' said Mr. Ubelmann, who founded the company Iconem. 'This is the stuff architects and archaeologists need. ' They need it in a new push for virtual preservation that scientists, archaeologists and others, like Mr. Ubelmann, are compiling on a large scale. The records could be used to create computer models that would show how monuments and endangered historical sites might one day be restored, repaired or reconstructed. Of special interest today are ancient sites in Syria, and also Iraq, that have suffered from war, looting and the Islamic State. 'Palmyra was very difficult,' Mr. Ubelmann said. 'The terrorists were uploading videos with them blowing up monuments and smashing statues to manipulate public opinion,' he said. 'We felt the best response was to magnify the pictures of these places and show their splendor and their importance to the culture. It became a war of images. ' The latest front in that war is in the exhibition halls of the Grand Palais in Paris, where, through Jan. 9, many of the 40, 000 images he and his team took at Palmyra have become the basis for displays. Called 'Eternal Sites: From Bamiyan to Palmyra,' the show aims to draw attention to the rising threats to global heritage. To underscore the exhibition's political importance, it was opened several weeks ago by President Fran'ois Hollande of France, who described it as 'an act of resistance' against terror and intolerance. Showing the beauty of the Middle Eastern heritage, he said, 'is the best answer to the Islamist propaganda of hate, destruction and death. ' Martinez, the director of the Louvre and the lead curator of the show, said the sites had been chosen because 'all are under threat from pillaging, neglect or destruction and are not accessible to the public. ' He said it aimed to mobilize public opinion 'in the face of the devastation of unique heritage. ' Besides images from Palmyra, the multimedia show projects enormous photographs and videos, immersing visitors in different eras, including the ancient Iraqi city of Khorsabad around 700 B. C. an mosque in Damascus and a medieval Christian citadel. Mr. Ubelmann dismissed any criticism of collaboration with the government of the Syrian president, Bashar . 'We were working pro bono, not for any government, but to help the archaeologists,' he said. They shared their work with the Syrian archaeologists, he said, adding, 'We also train our colleagues so they can later do this on their own. ' What is paramount is memory and potential restoration. In the last year, his team has flown drones over some 20 historic sites in Syria. Recently, it moved into zones in Iraq, close to the front line in the fight against the Islamic State. The team is now analyzing the war's effects on the remains of once thriving cities dating back some 3, 000 years, including Nineveh, Khorsabad and the thrashed temple and palace of Nimrud, where the government drove out the jihadists in November. In 2015, Islamists sent out videos showing militants using sledgehammers to break reliefs of human figures and mythical winged bulls as part of their campaign. 'Nimrud was probably the most splendid of the Assyrian cities,' Layla Abdulkarim, a Syrian architect, said as she analyzed aerial photographs. Using drones in archaeological work is not entirely new, specialists say, but at a recent gathering in Paris researchers from Europe and the Middle East said they were now having to practice 'war archaeology,' that is, collecting reliable data from areas. The images from the drones in war zones had proved immensely valuable. But these were barely scratching the surface. Before the war, close to 150 archaeological projects were underway, just in Syria, researchers said. Experts from many countries are trying to assess the damage in Syria's old cities but also in the area where the Islamic State held sway that is straddling Iraq and Syria, the region that is seen as central to human history and often called the birthplace of modern economics and writing. There is an outcry for data about the havoc wreaked in Yemen by Saudi bombing. 'People are exchanging satellite images and data on blogs and other research platforms, but we have no real assessment yet because so many ancient sites are not accessible,' said Pascal Butterlin, a professor of archaeology at the Sorbonne in Paris. Time is of the essence, even in the case of ruins, Mr. Butterlin said. He has led expeditions for more than 20 years to Mari, near Syria's border with Iraq. Before fleeing, the guards at Mari reported that looters had come from Iraq, he said. 'We need to know what places need to be stabilized and how looters have altered the sites,' he said. 'Important evidence, like clandestine pits, can disappear very quickly through sandstorms and erosion. ' Cheikhmous Ali, a Syrian archaeologist based in France, who founded the international group the Association for the Protection of Syrian Archaeology, said reports of organized pillaging continued. A first wave of looting began in 2012, Mr. Ali said, and looting has accelerated since 2014 with the arrival of the Islamic State. While jihadists were more motivated to destroy the artifacts, they had also allowed looters to operate in exchange for money. Mr. Ali said he kept an ever changing tally of museums bombed, objects carted off, safes stolen. The exhibition in Paris, which is drawing large crowds, coincides with 'History Begins in Mesopotamia,' a show at the Louvre's regional museum in Lens. Both exhibitions highlight the French government's active concern about cultural damage in Syria, which was briefly controlled by France in the first half of the 20th century. Mr. Hollande has taken a strong interest, condemning the deliberate destruction of patrimony by all sides as 'war crimes. ' This past month, France offered $30 million toward a proposed $100 million fund to protect sites as fighting abates, provide emergency storage for artifacts and eventually rehabilitate monuments. At the 'Eternal Sites' opening at the Grand Palais, Mr. Hollande stressed that France was taking in more Syrian refugees trying to protect monuments of great historical and cultural importance did not mean ignoring the suffering of the population. 'Should we be concerned about the patrimony'' he asked. 'What is more important, saving lives or saving stones' In reality, these two are inseparable. '\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed_article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17285</td>\n",
       "      <td>When Walt Disney's 'Bambi' opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17286</td>\n",
       "      <td>Death may be the great equalizer, but it isn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17297</td>\n",
       "      <td>It's the season for family travel and photos '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17306</td>\n",
       "      <td>Mariah Carey suffered through a performance tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17308</td>\n",
       "      <td>Pop music and fashion never met cuter than in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>18386</td>\n",
       "      <td>Senator Jeff Sessions was confirmed on Wednes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>18408</td>\n",
       "      <td>Was it just a year ago that Katie Holmes, Jenn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>18413</td>\n",
       "      <td>Mother Jones was named magazine of the year on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>18417</td>\n",
       "      <td>Damien Chazelle, the writer and director of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>18438</td>\n",
       "      <td>Because he was told that were box office pois...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                  processed_article\n",
       "19   17285  When Walt Disney's 'Bambi' opened in 1942, cri...\n",
       "20   17286  Death may be the great equalizer, but it isn't...\n",
       "21   17297  It's the season for family travel and photos '...\n",
       "22   17306  Mariah Carey suffered through a performance tr...\n",
       "23   17308  Pop music and fashion never met cuter than in ...\n",
       "..     ...                                                ...\n",
       "931  18386   Senator Jeff Sessions was confirmed on Wednes...\n",
       "951  18408  Was it just a year ago that Katie Holmes, Jenn...\n",
       "956  18413  Mother Jones was named magazine of the year on...\n",
       "959  18417  Damien Chazelle, the writer and director of th...\n",
       "978  18438   Because he was told that were box office pois...\n",
       "\n",
       "[153 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment_data = data[data['topic'] == 'entertainment']\n",
    "entertainment_subset = entertainment_data[['id', 'processed_article']]\n",
    "entertainment_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "nlp=spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fba3072a910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "test = data.loc[data['id'] == 17307, 'processed_article'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Syria: [Syria, Syria, the country],\n",
       " his: [his, he, his],\n",
       " mathematicians and designers: [mathematicians and designers, their],\n",
       " Palmyra: [Palmyra, it],\n",
       " This: [This, it],\n",
       " the stuff architects and archaeologists need: [the stuff architects and archaeologists need, They],\n",
       " Mr. Ubelmann: [Mr. Ubelmann, Mr. Ubelmann, he],\n",
       " Palmyra: [Palmyra, Palmyra, Palmyra, Palmyra],\n",
       " The terrorists: [The terrorists, them],\n",
       " the best response: [the best response, It, the show, it, it, the show, it, the multimedia show, it],\n",
       " these places: [these places, their, their],\n",
       " a war of images: [a war of images, that war],\n",
       " he: [he, his, he, He],\n",
       " global heritage: [global heritage, the Middle Eastern heritage],\n",
       " Mr. Ubelmann: [Mr. Ubelmann, he, he, his],\n",
       " We: [We, We, our],\n",
       " the archaeologists: [the archaeologists, the Syrian archaeologists],\n",
       " They: [They, their, their],\n",
       " our colleagues: [our colleagues, they],\n",
       " his team: [his team, The team],\n",
       " Islamists: [Islamists, their],\n",
       " Nimrud: [Nimrud, she],\n",
       " specialists: [specialists, they],\n",
       " The images from the drones in war zones: [The images from the drones in war zones, these],\n",
       " Syria: [Syria, Syria, Syria, Syria],\n",
       " Iraq: [Iraq, Iraq, Iraq],\n",
       " Mr. Butterlin: [Pascal Butterlin, a professor of archaeology at the Sorbonne in Paris, Mr. Butterlin, He, he, he],\n",
       " Mari: [Mari, Mari],\n",
       " Mr. Ali: [Cheikhmous Ali, a Syrian archaeologist based in France, who founded the international group the Association for the Protection of Syrian Archaeology, Mr. Ali, Mr. Ali, he, Mr. Hollande, Mr. Hollande, he],\n",
       " France: [France, France, France, France],\n",
       " jihadists: [jihadists, they],\n",
       " France was taking in more Syrian refugees trying to protect monuments of great historical and cultural importance: [France was taking in more Syrian refugees trying to protect monuments of great historical and cultural importance, we]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(test)\n",
    "\n",
    "doc._.has_coref\n",
    "coref_clusters=doc._.coref_clusters\n",
    "coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticle(number):\n",
    "     return data.loc[data['id'] ==number, 'processed_article'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_cluster():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "    coref_clusters = None  # Define coref_clusters at the class level\n",
    "\n",
    "    @staticmethod\n",
    "    def cluster(context):\n",
    "        doc = create_cluster.nlp(context)  # Access nlp using class name\n",
    "        if doc._.has_coref:\n",
    "            create_cluster.coref_clusters = doc._.coref_clusters  # Store clusters in class attribute\n",
    "\n",
    "    @staticmethod\n",
    "    def getClusters():\n",
    "        return create_cluster.coref_clusters  # Access coref_clusters using class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def extract_relevant_sentences_hf(context, question,num_context_sentences=0):\n",
    "    model_name = \"paraphrase-MiniLM-L6-v2\"  \n",
    "    model = SentenceTransformer(model_name)\n",
    "    create_cluster.cluster(context)\n",
    "    context_sentences = nltk.sent_tokenize(context)\n",
    "    # Encode the question and context\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    context_embeddings = model.encode(context_sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity between question and context sentences\n",
    "    cos_similarities = util.pytorch_cos_sim(question_embedding, context_embeddings)[0]\n",
    "\n",
    "    cos_similarities = util.pytorch_cos_sim(question_embedding, context_embeddings)[0]\n",
    "\n",
    "    # Find the index of the sentence with the maximum similarity\n",
    "    max_similarity_index = cos_similarities.argmax().item()\n",
    "\n",
    "    start_index = max(0, max_similarity_index - num_context_sentences)\n",
    "    end_index = min(len(context_sentences) - 1, max_similarity_index + num_context_sentences)\n",
    "\n",
    "    context_paragraph = ' '.join(context_sentences[start_index:end_index + 1])\n",
    "\n",
    "    return context_paragraph #cos_similarities[max_similarity_index].item()\n",
    "def extract_relevant_sentences_tfidf(context, question, num_context_sentences=3):\n",
    "    # Tokenize context into sentences\n",
    "    create_cluster.cluster(context)\n",
    "    context_sentences = nltk.sent_tokenize(context)\n",
    "\n",
    "    # Tokenize question into words\n",
    "    question_words = nltk.word_tokenize(question.lower())\n",
    "\n",
    "    # Vectorize context sentences using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(context_sentences)\n",
    "\n",
    "    # Calculate TF-IDF vector for the question\n",
    "    question_tfidf = tfidf_vectorizer.transform([' '.join(question_words)])\n",
    "\n",
    "    # Calculate cosine similarity between question TF-IDF vector and context TF-IDF matrix\n",
    "    cosine_similarities = cosine_similarity(question_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Find the indices of top N most similar sentences\n",
    "    top_indices = cosine_similarities.argsort()[-num_context_sentences:][::-1]\n",
    "\n",
    "    # Extract relevant sentences\n",
    "    relevant_paragraph = ' '.join([context_sentences[i] for i in top_indices])\n",
    "\n",
    "    return relevant_paragraph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "money_words = [\n",
    "    'dollar', 'euro', 'pound', 'yen', 'payment', 'price', 'cost', 'fee', 'salary',\n",
    "    'profit', 'revenue', 'income', 'expenditure', 'budget',\n",
    "    'bank', 'account', 'credit', 'loan', 'interest', 'GDP', 'inflation', 'deflation',\n",
    "    'exchange rate', 'stock', 'bond', 'dividend', 'capital', 'tax', 'duty', 'levy',\n",
    "    'refund', 'revenue', 'expense', 'loss', 'cash', 'credit card', 'check', 'PayPal','amount'\n",
    "]\n",
    "date_related_words = [\n",
    "    'celebration', 'anniversary', 'ceremony', 'festival', 'holiday', 'occasion', 'commemoration',\n",
    "    'era', 'century', 'decade', 'generation', 'period', 'epoch', 'age',\n",
    "    'timeline', 'chronology', 'schedule', 'agenda', 'timetable', 'deadline', 'due date', 'expiry date',\n",
    "    'duration', 'timeframe', 'interval', 'period', 'length', 'span', 'term', 'periodicity',\n",
    "    'yesterday', 'today', 'tomorrow', 'past', 'present', 'future', 'then', 'now',\n",
    "    'spring', 'summer', 'autumn', 'winter', 'monsoon', 'rainy season', 'dry season', 'harvest season',\n",
    "    'date', 'day', 'week', 'month', 'year', 'decade', 'century', 'millennium',\n",
    "    'age', 'youth', 'aging', 'seniority', 'life stage'\n",
    "]\n",
    "person_question_words = ['who', 'whose', 'whom', 'name', 'identity', 'character', 'individual', 'subject', 'figure', 'protagonist']\n",
    "\n",
    "gpe_question_words = ['where', 'location', 'place', 'country', 'city', 'state', 'region', 'province', 'town', 'area','capital']\n",
    "\n",
    "def answertype(article, question):\n",
    "\n",
    "    context=extract_relevant_sentences_tfidf(article,question)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    if questiontype(question):\n",
    "        t = 'DESCRIPTIVE'\n",
    "        word = word_tokenize(question.lower())\n",
    "        if any(x in word for x in person_question_words):\n",
    "            t = 'PERSON'\n",
    "        elif 'where' in word:\n",
    "            t = 'GPE'\n",
    "        elif any(x in word for x in date_related_words):\n",
    "            t = 'DATE'\n",
    "        elif 'when' in word or any(x in word for x in date_related_words):\n",
    "            t = 'DATE'\n",
    "        elif any(x in word for x in money_words):\n",
    "            t= 'MONEY'\n",
    "        \n",
    "        coref_clusters=create_cluster.getClusters()\n",
    "        # Perform further processing based on answer type\n",
    "        if t == 'DESCRIPTIVE':\n",
    "            # Perform descriptive processing\n",
    "            # For now, return the question itself\n",
    "            return question\n",
    "        \n",
    "        else:\n",
    "            # Perform processing based on entity extraction\n",
    "            # Extract entities of the identified answer type from the context\n",
    "            \n",
    "            \n",
    "            doc = nlp(context)\n",
    "            \n",
    "            entities = [ent.text for ent in doc.ents if ent.label_ == t]\n",
    "            #events = extract_events(doc)  # Extract events from the context\n",
    "            #relations = extract_relations(doc)  # Extract relations between entities\n",
    "            \n",
    "            if coref_clusters:\n",
    "                # Check if the first entity is in any head cluster\n",
    "                first_entity = entities[0] if entities else None\n",
    "                for cluster in coref_clusters:\n",
    "                    if first_entity and first_entity in cluster.main.text:\n",
    "                        return cluster.main.text  # Return the head of the coreference cluster\n",
    "\n",
    "            # If entities of the answer type are found, return the first one\n",
    "            if entities:\n",
    "                return entities[0]\n",
    "            #elif events:\n",
    "              #  return events[0]\n",
    "           # elif relations:\n",
    "               # return relations[0]\n",
    "            else:\n",
    "                # If no entities, events, or relations are found, return the context itself\n",
    "                return \"Couldn't find the answer in the context.\"\n",
    "\n",
    "    else:\n",
    "        # Perform descriptive processing\n",
    "        # For now, return the question itself\n",
    "        return \"Not A Question!\"\n",
    "\n",
    "\n",
    "def questiontype(question):\n",
    "    # Define question tags\n",
    "    questiontags = ['WP', 'WDT', 'WP$', 'WRB']\n",
    "\n",
    "    # Tokenize and tag the question\n",
    "    question_POS = pos_tag(word_tokenize(question.lower()))\n",
    "    \n",
    "    # Identify question tags\n",
    "    question_tags = [token for token, tag in question_POS if tag in questiontags]\n",
    "\n",
    "    # Check if there is only one question tag and it's not \"what\"\n",
    "    if len(question_tags) == 1 :\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#def extract_events(doc):\n",
    " #   events = []\n",
    "  #  for token in doc:\n",
    "   #     if token.pos_ == \"VERB\":\n",
    "    #        events.append(token.text)\n",
    "    #return events\n",
    "\n",
    "#def extract_relations(doc):\n",
    " #   relations = []\n",
    "  #  for token in doc:\n",
    "   #     if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "    #        # Extract relation in the form of subject-verb\n",
    "     #       relation = f\"{token.text}-{token.head.text}\"\n",
    "      #      relations.append(relation)\n",
    "    #return relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Mr. Ubelmann\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example relevant context and question\n",
    "question = \"Who founded the company Iconem\"\n",
    "relevant=extract_relevant_sentences_tfidf(test,question)\n",
    "#print(f\"Most similar sentence: {relevant}\")\n",
    "#print(f\"Similarity score: \")\n",
    "\n",
    "# Extract the answer\n",
    "answer = answertype(getArticle(17307), question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions=pd.read_csv(\"new_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yves Ubelmann\n",
      "Mr. Ubelmann\n",
      "Martinez\n",
      "Louvre\n",
      "Houmam Saad\n",
      "Ubelmann\n",
      "President François Hollande\n",
      "Palmyra\n",
      "President François Hollande\n",
      "Couldn't find the answer in the context.\n",
      "Yves Ubelmann\n",
      "Couldn't find the answer in the context.\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "President François Hollande\n",
      "Hollande\n",
      "France\n",
      "Couldn't find the answer in the context.\n",
      "Zoraida Mu'iz and her family\n",
      "Couldn't find the answer in the context.\n",
      "Zoraida Mu'iz\n",
      "Not A Question!\n",
      "Jos' Mu'iz\n",
      "Ms. Mu'iz\n",
      "Jos' Jr.\n",
      "Jos' Jr.\n",
      "Jesus\n",
      "Jesus\n",
      "Jos' Mu'iz\n",
      "Couldn't find the answer in the context.\n",
      "Surgeons\n",
      "Mr. Mu'iz\n",
      "Jesus\n",
      "Jesus\n",
      "Zoraida Mu'iz\n",
      "Ms. Mu'iz\n",
      "Maria\n",
      "Couldn't find the answer in the context.\n",
      "Precision Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize lists to store expected and extracted answers\n",
    "expected_answers = []\n",
    "extracted_answers = []\n",
    "\n",
    "for index, row in test_questions.iterrows():\n",
    "    question = row['question']\n",
    "    article_id = row['article']\n",
    "    expected_answer = row['answer']\n",
    "    \n",
    "    # Fetch the article based on the article_id\n",
    "    test_article = data.loc[data['id'] == article_id, 'processed_article'].values[0]\n",
    "    \n",
    "    # Extract relevant sentences from the article\n",
    "    relevant_sentences = extract_relevant_sentences_hf(test_article, question)\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer = answertype(relevant_sentences, question)\n",
    "    \n",
    "    # Append expected and extracted answers to the lists\n",
    "    expected_answers.append(expected_answer)\n",
    "    print(expected_answer)\n",
    "    extracted_answers.append(answer)\n",
    "    print(answer)\n",
    "\n",
    "# Compute the precision score\n",
    "precision = f1_score(expected_answers, extracted_answers, average='micro')\n",
    "\n",
    "print(\"Precision Score:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yves Ubelmann\n",
      "Ubelmann\n",
      "Martinez\n",
      "Martinez\n",
      "Houmam Saad\n",
      "Cheikhmous Ali\n",
      "President François Hollande\n",
      "Palmyra\n",
      "President François Hollande\n",
      "Couldn't find the answer in the context.\n",
      "Yves Ubelmann\n",
      "Nineveh\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "Cheikhmous Ali\n",
      "President François Hollande\n",
      "Mr. Hollande\n",
      "France\n",
      "Palmyra\n",
      "Zoraida Mu'iz and her family\n",
      "Dickens\n",
      "Zoraida Mu'iz\n",
      "Not A Question!\n",
      "Jos' Mu'iz\n",
      "Mu'iz\n",
      "Jos' Jr.\n",
      "Jesus\n",
      "Jesus\n",
      "Jesus\n",
      "Jos' Mu'iz\n",
      "Ms. Mu'iz\n",
      "Surgeons\n",
      "Mr. Mu'iz\n",
      "Jesus\n",
      "Jesus\n",
      "Zoraida Mu'iz\n",
      "Mu'iz\n",
      "Maria\n",
      "Maria\n",
      "Precision Score: 0.3\n"
     ]
    }
   ],
   "source": [
    "expected_answers = []\n",
    "extracted_answers = []\n",
    "\n",
    "for index, row in test_questions.iterrows():\n",
    "    question = row['question']\n",
    "    article_id = row['article']\n",
    "    expected_answer = row['answer']\n",
    "    \n",
    "    # Fetch the article based on the article_id\n",
    "    test_article = data.loc[data['id'] == article_id, 'processed_article'].values[0]\n",
    "    \n",
    "    # Extract relevant sentences from the article\n",
    "    relevant_sentences = extract_relevant_sentences_tfidf(test_article, question)\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer = answertype(relevant_sentences, question)\n",
    "    \n",
    "    # Append expected and extracted answers to the lists\n",
    "    expected_answers.append(expected_answer)\n",
    "    print(expected_answer)\n",
    "    extracted_answers.append(answer)\n",
    "    print(answer)\n",
    "\n",
    "# Compute the precision score\n",
    "precision = f1_score(expected_answers, extracted_answers, average='micro')\n",
    "\n",
    "print(\"Precision Score:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "Question: In what country is Normandy located?\n",
      "Predicted Answer: In what country is Normandy located?\n",
      "Plausible Answer: France\n",
      "Question: When were the Normans in Normandy?\n",
      "Predicted Answer: the first half of the 10th century\n",
      "Plausible Answer: 10th and 11th centuries\n",
      "Question: From which countries did the Norse originate?\n",
      "Predicted Answer: From which countries did the Norse originate?\n",
      "Plausible Answer: Denmark, Iceland and Norway\n",
      "Question: Who was the Norse leader?\n",
      "Predicted Answer: Norse\n",
      "Plausible Answer: Rollo\n",
      "Question: What century did the Normans first gain their separate identity?\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: 10th century\n",
      "Question: Who gave their name to Normandy in the 1000's and 1100's\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: Normans\n",
      "Question: What is France a region of?\n",
      "Predicted Answer: What is France a region of?\n",
      "Plausible Answer: Normandy\n",
      "Question: Who did King Charles III swear fealty to?\n",
      "Predicted Answer: Norse\n",
      "Plausible Answer: Rollo\n",
      "Question: When did the Frankish identity emerge?\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: 10th century\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming squad_test[\"data\"] contains the JSON data\n",
    "with open(\"SQUAD-Testing.json\", \"r\") as f:\n",
    "    squad_test = json.load(f)\n",
    "\n",
    "# Iterate through the \"data\" array\n",
    "for data_item in squad_test[\"data\"]:\n",
    "    # Read the context\n",
    "    context = data_item[\"context\"]\n",
    "    print(\"Context:\", context)\n",
    "\n",
    "    # Iterate through the questions\n",
    "    for question_item in data_item[\"questions\"]:\n",
    "        question = question_item[\"question\"]\n",
    "        print(\"Question:\", question)\n",
    "        predicted_answer=answertype(context,question)\n",
    "        # You can also access the plausible answers if needed\n",
    "        plausible_answers = question_item[\"plausible_answers\"]\n",
    "        for answer in plausible_answers:\n",
    "            answer_text = answer[\"text\"]\n",
    "            print(\"Predicted Answer:\",predicted_answer)\n",
    "            print(\"Plausible Answer:\", answer_text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70506ed34e584b3b8ba37e51f92da361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', placeholder='Enter your question')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd15b28e45049a9b9f54262b4134cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Article Number:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda0db264974490ab91b123ddf8b8b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81597409e46d403eab63cf7e713e1618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Get Answer', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display,clear_output\n",
    "\n",
    "# Define input widgets\n",
    "question_input = widgets.Text(description='Question:', placeholder='Enter your question', disabled=False)\n",
    "article_input = widgets.IntText(description='Article Number:', value=0, disabled=False)\n",
    "answer_output=widgets.Output(description=\"Answer\")\n",
    "# Define a function to handle user input and display the answer\n",
    "\n",
    "\n",
    "# Define a callback function to be called when the button is clicked\n",
    "def on_button_click(b):\n",
    "    question = question_input.value\n",
    "    article_number = article_input.value\n",
    "    if question and article_number:\n",
    "        article=getArticle(article_number)\n",
    "    else:\n",
    "        print(\"Give Proper Questiona and Article\")\n",
    "    with answer_output:\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    # Display the answer\n",
    "    with answer_output:\n",
    "        answer = answertype(article, question)\n",
    "        print(\"Answer:\",answer)\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(description='Get Answer')\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the input widgets and button\n",
    "display(question_input, article_input,answer_output, button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "Question: In what country is Normandy located?\n",
      "Predicted Answer: In what country is Normandy located?\n",
      "Plausible Answer: France\n",
      "Question: When were the Normans in Normandy?\n",
      "Predicted Answer: the first half of the 10th century\n",
      "Plausible Answer: 10th and 11th centuries\n",
      "Question: From which countries did the Norse originate?\n",
      "Predicted Answer: From which countries did the Norse originate?\n",
      "Plausible Answer: Denmark, Iceland and Norway\n",
      "Question: Who was the Norse leader?\n",
      "Predicted Answer: Norse\n",
      "Plausible Answer: Rollo\n",
      "Question: What century did the Normans first gain their separate identity?\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: 10th century\n",
      "Question: Who gave their name to Normandy in the 1000's and 1100's\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: Normans\n",
      "Question: What is France a region of?\n",
      "Predicted Answer: What is France a region of?\n",
      "Plausible Answer: Normandy\n",
      "Question: Who did King Charles III swear fealty to?\n",
      "Predicted Answer: Norse\n",
      "Plausible Answer: Rollo\n",
      "Question: When did the Frankish identity emerge?\n",
      "Predicted Answer: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)\n",
      "Plausible Answer: 10th century\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming squad_test[\"data\"] contains the JSON data\n",
    "with open(\"SQUAD-Testing.json\", \"r\") as f:\n",
    "    squad_test = json.load(f)\n",
    "\n",
    "# Iterate through the \"data\" array\n",
    "for data_item in squad_test[\"data\"]:\n",
    "    # Read the context\n",
    "    context = data_item[\"context\"]\n",
    "    print(\"Context:\", context)\n",
    "\n",
    "    # Iterate through the questions\n",
    "    for question_item in data_item[\"questions\"]:\n",
    "        question = question_item[\"question\"]\n",
    "        print(\"Question:\", question)\n",
    "        predicted_answer=answertype(context,question)\n",
    "        # You can also access the plausible answers if needed\n",
    "        plausible_answers = question_item[\"plausible_answers\"]\n",
    "        for answer in plausible_answers:\n",
    "            answer_text = answer[\"text\"]\n",
    "            print(\"Predicted Answer:\",predicted_answer)\n",
    "            print(\"Plausible Answer:\", answer_text)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
